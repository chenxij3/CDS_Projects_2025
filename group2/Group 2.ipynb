{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.model_selection import KFold, cross_val_score"
      ],
      "metadata": {
        "id": "bEAhmxfqaXYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -R /content"
      ],
      "metadata": {
        "id": "UpttTv_AoyYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fac6b2b-cb18-470f-97f0-90779e115cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content:\n",
            "sample_data\n",
            "\n",
            "/content/sample_data:\n",
            "anscombe.json\t\t      mnist_test.csv\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BASELINE MODEL"
      ],
      "metadata": {
        "id": "u3yomAYCffXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"OPV_dataset_quiz_training.csv\")\n",
        "#df = pd.read_csv(\"/content/sample_data/OPV_dataset_quiz_training.csv\")\n",
        "\n",
        "target_col = \"PCE\"\n",
        "df = df.dropna(subset=[target_col])\n",
        "\n",
        "X = df.drop(columns=[target_col]).select_dtypes(include=[np.number])\n",
        "y = df[target_col]\n",
        "\n",
        "# preprocessing stuff\n",
        "print(\"Missing values per feature BEFORE cleaning:\")\n",
        "print(X.isna().sum())\n",
        "\n",
        "# 1. Drop rows where ANY feature value is NaN\n",
        "clean_df = df.dropna(axis=0)   # keeps only fully complete rows\n",
        "\n",
        "# 2. Recompute X and y so all downstream code stays identical\n",
        "X = clean_df.drop(columns=[target_col]).select_dtypes(include=[np.number])\n",
        "y = clean_df[target_col]\n",
        "\n",
        "print(\"\\nAFTER CLEANING SHAPES:\")\n",
        "print(\"X:\", X.shape)\n",
        "print(\"y:\", y.shape)\n",
        "\n",
        "# 3. 5-Fold Cross Validation with baseline RF\n",
        "rf_cv = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "neg_rmse_scores = cross_val_score(\n",
        "    rf_cv,\n",
        "    X, y,\n",
        "    cv=cv,\n",
        "    scoring=\"neg_root_mean_squared_error\"\n",
        ")\n",
        "rmse_scores = -neg_rmse_scores\n",
        "\n",
        "r2_scores = cross_val_score(\n",
        "    rf_cv,\n",
        "    X, y,\n",
        "    cv=cv,\n",
        "    scoring=\"r2\"\n",
        ")\n",
        "\n",
        "print(\"\\n5-fold Cross-Validation (Baseline RF)\")\n",
        "print(f\"RMSE (mean ± std): {rmse_scores.mean():.4f} ± {rmse_scores.std():.4f}\")\n",
        "print(f\"R^2  (mean ± std): {r2_scores.mean():.4f} ± {r2_scores.std():.4f}\")\n",
        "#end of preprocessing stuff\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf.predict(X_val)\n",
        "r2_val = r2_score(y_val, y_pred)\n",
        "rmse_val = mean_squared_error(y_val, y_pred) ** 0.5\n",
        "\n",
        "print(f\"Train size: {X_train.shape[0]}\")\n",
        "print(f\"Validation size: {X_val.shape[0]}\")\n",
        "print(f\"Validation R^2: {r2_val:.4f}\")\n",
        "print(f\"Validation RMSE: {rmse_val:.4f}\")\n"
      ],
      "metadata": {
        "id": "QAff66vEbrzL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1429ac1-4691-446f-dfac-e555c6200236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values per feature BEFORE cleaning:\n",
            "ID No.                0\n",
            "Voc (V)               2\n",
            "Jsc (mA cm^2)         0\n",
            "FF                    1\n",
            "Mw (kg mol^-1)        1\n",
            "Mn (kg mol^-1)        1\n",
            "PDI (=Mw/Mn)          1\n",
            "Monomer (g mol^-1)    0\n",
            "-HOMO (eV)            0\n",
            "-LUMO (eV)            1\n",
            "bandgap(eV)           1\n",
            "dtype: int64\n",
            "\n",
            "AFTER CLEANING SHAPES:\n",
            "X: (960, 11)\n",
            "y: (960,)\n",
            "\n",
            "===== 5-fold Cross-Validation (Baseline RF) =====\n",
            "RMSE (mean ± std): 0.3598 ± 0.0318\n",
            "R^2  (mean ± std): 0.9761 ± 0.0041\n",
            "Train size: 768\n",
            "Validation size: 192\n",
            "Validation R^2: 0.9821\n",
            "Validation RMSE: 0.3172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENETIC ALGORITHM TUNING"
      ],
      "metadata": {
        "id": "wCayZ94KfhTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit\n",
        "\n",
        "import rdkit\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem, Draw\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from sklearn.linear_model import LinearRegression, Lasso, LogisticRegression, Ridge\n",
        "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, LeaveOneOut, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree, export_graphviz\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, StackingRegressor\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from rdkit.DataStructs import TanimotoSimilarity\n",
        "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.initializers import Zeros, RandomUniform, GlorotUniform\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import shap\n",
        "import random"
      ],
      "metadata": {
        "id": "_xsFM_9Z1aVI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b33218d-3dbb-446b-94b2-2734a94a652e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.12/dist-packages (2025.9.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Genetic Algorithm Tuning for RandomForestRegressor\n",
        "\n",
        "train_df = pd.read_csv(\"/content/sample_data/OPV_dataset_quiz_training.csv\")\n",
        "test_df  = pd.read_csv(\"/content/sample_data/OPV_dataset_quiz_test.csv\")\n",
        "\n",
        "#train_df = pd.read_csv(\"/content/OPV_dataset_quiz_training.csv\")\n",
        "#test_df  = pd.read_csv(\"/content/OPV_dataset_quiz_test.csv\")\n",
        "\n",
        "target_col = \"PCE\"\n",
        "train_df = train_df.dropna(subset=[target_col])\n",
        "\n",
        "X = train_df.drop(columns=[target_col]).select_dtypes(include=[np.number])\n",
        "y = train_df[target_col]\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Corrected Hyperparameter Space\n",
        "param_space = {\n",
        "    \"n_estimators\": [50, 100, 200, 300, 400],\n",
        "    \"max_depth\": [None, 5, 10, 20, 40],\n",
        "    \"min_samples_split\": [2, 4, 6, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4],\n",
        "    \"max_features\": [\"sqrt\", \"log2\", None]\n",
        "}\n",
        "\n",
        "# Random individual generator\n",
        "def random_individual():\n",
        "    return {\n",
        "        \"n_estimators\": random.choice(param_space[\"n_estimators\"]),\n",
        "        \"max_depth\": random.choice(param_space[\"max_depth\"]),\n",
        "        \"min_samples_split\": random.choice(param_space[\"min_samples_split\"]),\n",
        "        \"min_samples_leaf\": random.choice(param_space[\"min_samples_leaf\"]),\n",
        "        \"max_features\": random.choice(param_space[\"max_features\"])\n",
        "    }\n",
        "\n",
        "# Fitness = RMSE\n",
        "def fitness(individual):\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=individual[\"n_estimators\"],\n",
        "        max_depth=individual[\"max_depth\"],\n",
        "        min_samples_split=individual[\"min_samples_split\"],\n",
        "        min_samples_leaf=individual[\"min_samples_leaf\"],\n",
        "        max_features=individual[\"max_features\"],\n",
        "        random_state=0,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_val)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val, preds))  # Fixed\n",
        "    return rmse\n",
        "\n",
        "# Crossover\n",
        "def crossover(p1, p2):\n",
        "    return {k: random.choice([p1[k], p2[k]]) for k in p1}\n",
        "\n",
        "# Mutation\n",
        "def mutate(individual, mutation_rate=0.2):\n",
        "    for key in individual:\n",
        "        if random.random() < mutation_rate:\n",
        "            individual[key] = random.choice(param_space[key])\n",
        "    return individual\n",
        "\n",
        "\n",
        "# Genetic Algorithm Loop\n",
        "population_size = 20\n",
        "generations = 10\n",
        "population = [random_individual() for _ in range(population_size)]\n",
        "ga_best_rmse_history = [] #added this to store the best RMSE per generation in a list - hita\n",
        "for gen in range(generations):\n",
        "    print(f\"\\n Generation {gen+1}\")\n",
        "\n",
        "    scored_pop = []\n",
        "    for indiv in population:\n",
        "        rmse = fitness(indiv)\n",
        "        scored_pop.append((rmse, indiv))\n",
        "\n",
        "    scored_pop.sort(key=lambda x: x[0])\n",
        "\n",
        "    best_rmse_this_gen = scored_pop[0][0] #added this - hita\n",
        "    ga_best_rmse_history.append(best_rmse_this_gen) #added this - hita\n",
        "\n",
        "    print(f\"Best RMSE this generation: {best_rmse_this_gen:.4f}\")\n",
        "\n",
        "    survivors = [ind for (_, ind) in scored_pop[:population_size // 2]]\n",
        "    new_population = survivors.copy()\n",
        "    while len(new_population) < population_size:\n",
        "        parents = random.sample(survivors, 2)\n",
        "        child = crossover(parents[0], parents[1])\n",
        "        child = mutate(child)\n",
        "        new_population.append(child)\n",
        "    population = new_population\n",
        "\n",
        "best_rmse, best_params = scored_pop[0]\n",
        "print(\"\\nBest Hyperparameters Found:\")\n",
        "print(best_params)\n",
        "print(f\"\\nBest Validation RMSE: {best_rmse:.4f}\")\n",
        "\n",
        "# Train final model\n",
        "final_model = RandomForestRegressor(\n",
        "    **best_params,\n",
        "    random_state=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "final_model.fit(X, y)\n",
        "\n",
        "# Make sure test features match training features\n",
        "X_test = test_df.drop(columns=[target_col], errors='ignore').select_dtypes(include=[np.number])\n",
        "test_predictions = final_model.predict(X_test)\n",
        "print(\"\\nFinal GA-Optimized RandomForest Model Trained.\")\n",
        "\n",
        "# Save predictions\n",
        "submission = pd.DataFrame({\n",
        "    \"ID\": test_df.index,\n",
        "    \"Predicted_PCE\": test_predictions\n",
        "})\n",
        "submission.to_csv(\"GA_RF_predictions.csv\", index=False)\n",
        "print(\"Saved GA_RF_predictions.csv\")"
      ],
      "metadata": {
        "id": "URJxPbzk7mu1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c0378ce-add8-4bf8-cbdb-0329aba3f624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generation 1 ===\n",
            "Best RMSE this generation: 0.3509\n",
            "\n",
            "=== Generation 2 ===\n",
            "Best RMSE this generation: 0.3283\n",
            "\n",
            "=== Generation 3 ===\n",
            "Best RMSE this generation: 0.3283\n",
            "\n",
            "=== Generation 4 ===\n",
            "Best RMSE this generation: 0.3283\n",
            "\n",
            "=== Generation 5 ===\n",
            "Best RMSE this generation: 0.3283\n",
            "\n",
            "=== Generation 6 ===\n",
            "Best RMSE this generation: 0.3253\n",
            "\n",
            "=== Generation 7 ===\n",
            "Best RMSE this generation: 0.3245\n",
            "\n",
            "=== Generation 8 ===\n",
            "Best RMSE this generation: 0.3245\n",
            "\n",
            "=== Generation 9 ===\n",
            "Best RMSE this generation: 0.3244\n",
            "\n",
            "=== Generation 10 ===\n",
            "Best RMSE this generation: 0.3244\n",
            "\n",
            "=============================\n",
            "Best Hyperparameters Found:\n",
            "{'n_estimators': 300, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None}\n",
            "Best Validation RMSE: 0.3244\n",
            "=============================\n",
            "\n",
            "Final GA-Optimized RandomForest Model Trained.\n",
            "Saved GA_RF_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BAYESIAN OPTIMIZATION"
      ],
      "metadata": {
        "id": "0YTiwd-TfkFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bayesian-optimization\n",
        "from bayes_opt import BayesianOptimization"
      ],
      "metadata": {
        "id": "qVrJ-Ecvt3o5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define search space for Bayesian Optimization\n",
        "\n",
        "# RandomForest hyperparameter ranges\n",
        "pbounds = {\n",
        "    'n_estimators': (50, 400),\n",
        "    'max_depth': (5, 40),\n",
        "    'min_samples_split': (2, 10),\n",
        "    'min_samples_leaf': (1, 4),\n",
        "}\n",
        "\n",
        "\n",
        "# Objective function: negative RMSE\n",
        "\n",
        "\n",
        "def rf_cv(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n",
        "    n_estimators = int(round(n_estimators))\n",
        "    max_depth = int(round(max_depth))\n",
        "    min_samples_split = int(round(min_samples_split))\n",
        "    min_samples_leaf = int(round(min_samples_leaf))\n",
        "\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features='sqrt',\n",
        "        random_state=0,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # 5-fold CV on training set\n",
        "    scores = cross_val_score(model, X, y, cv=5, scoring='neg_root_mean_squared_error')\n",
        "    return scores.mean()\n",
        "\n",
        "\n",
        "# Run Bayesian Optimization\n",
        "\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=rf_cv,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Run 30 trials\n",
        "optimizer.maximize(init_points=5, n_iter=30)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_bo_params = optimizer.max['params']\n",
        "# Round integer hyperparameters\n",
        "best_bo_params = {\n",
        "    'n_estimators': int(round(best_bo_params['n_estimators'])),\n",
        "    'max_depth': int(round(best_bo_params['max_depth'])),\n",
        "    'min_samples_split': int(round(best_bo_params['min_samples_split'])),\n",
        "    'min_samples_leaf': int(round(best_bo_params['min_samples_leaf'])),\n",
        "    'max_features': 'sqrt'\n",
        "}\n",
        "\n",
        "print(\"Best Bayesian Hyperparameters Found:\")\n",
        "print(best_bo_params)\n",
        "\n",
        "\n",
        "# Train final Bayesian-optimized model\n",
        "\n",
        "\n",
        "final_bo_model = RandomForestRegressor(\n",
        "    **best_bo_params,\n",
        "    random_state=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "#shouldn't we be using X_train and y_train instead of X and y\n",
        "#final_bo_model.fit(X, y) (this is the original line)\n",
        "final_bo_model.fit(X_train, y_train) #i changed it to this - hita\n",
        "\n",
        "# Ensure test features match training features\n",
        "X_test = test_df.drop(columns=[target_col], errors='ignore').select_dtypes(include=[np.number])\n",
        "bo_test_preds = final_bo_model.predict(X_test)\n",
        "\n",
        "# Evaluate on validation set\n",
        "y_val_pred = final_bo_model.predict(X_val)\n",
        "r2_val = r2_score(y_val, y_val_pred)\n",
        "rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "print(f\"\\nBayesian-Optimized RandomForest Performance on Validation Set:\")\n",
        "print(f\"R² = {r2_val:.4f}\")\n",
        "print(f\"RMSE = {rmse_val:.4f}\")\n",
        "\n",
        "# Save predictions\n",
        "submission_bo = pd.DataFrame({\n",
        "    \"ID\": test_df.index,\n",
        "    \"Predicted_PCE\": bo_test_preds\n",
        "})\n",
        "submission_bo.to_csv(\"Bayes_RF_predictions.csv\", index=False)\n",
        "print(\"Saved Bayes_RF_predictions.csv\")"
      ],
      "metadata": {
        "id": "FN471TeJ_Yev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMPARE RESULTS"
      ],
      "metadata": {
        "id": "vQwtbdC7foJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot 1: GA fitness vs generation\n",
        "generations_axis = range(1, len(ga_best_rmse_history) + 1)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(generations_axis, ga_best_rmse_history, marker='o')\n",
        "plt.xlabel(\"Generation\")\n",
        "plt.ylabel(\"Best RMSE\")\n",
        "plt.title(\"Genetic Algorithm: Best RMSE per Generation\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wnbcQPRGzDZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot 2: Bayesian optimization score vs iteration\n",
        "# Extract RMSE for each iteration (convert negative target -> RMSE)\n",
        "bo_targets = [res[\"target\"] for res in optimizer.res]  # negative RMSE\n",
        "bo_rmse = -np.array(bo_targets)\n",
        "\n",
        "iterations = range(1, len(bo_rmse) + 1)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(iterations, bo_rmse, marker='o')\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"RMSE (CV)\")\n",
        "plt.title(\"Bayesian Optimization: RMSE per Iteration\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JLiifXUH4zHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot 3: Predicted vs Actual PCE (best model)\n",
        "#If best model is baseline RF:\n",
        "best_model = rf          # or final_ga_model or final_bo_model\n",
        "y_val_pred_best = best_model.predict(X_val)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(y_val, y_val_pred_best)\n",
        "min_val = min(y_val.min(), y_val_pred_best.min())\n",
        "max_val = max(y_val.max(), y_val_pred_best.max())\n",
        "plt.plot([min_val, max_val], [min_val, max_val])  # y = x reference line\n",
        "plt.xlabel(\"Actual PCE\")\n",
        "plt.ylabel(\"Predicted PCE\")\n",
        "plt.title(\"Predicted vs Actual PCE (Best Model)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kbdK7O_547Zy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}